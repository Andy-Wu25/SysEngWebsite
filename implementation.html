<!DOCTYPE html>
<html lang="en" class="no-js" >
<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Implementation</title>

    <script>
        document.documentElement.classList.remove('no-js');
        document.documentElement.classList.add('js');
    </script>

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="css/vendor.css">
    <link rel="stylesheet" href="css/styles.css">

    <!-- favicons
    ================================================== -->
    <link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="manifest" href="site.webmanifest">

</head>


<body id="top">

    
    <!-- preloader
    ================================================== -->
    <div id="preloader">
        <div id="loader" class="dots-fade">
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>


    <!-- page wrap
    ================================================== -->
    <div id="page" class="s-pagewrap ss-home">


          <!-- # site header 
        ================================================== -->
        <header class="s-header">

            <div class="row s-header__inner width-sixteen-col">

                <div class="s-header__block">
                    <div class="s-header__logo">
                        <a class="logo" href="index.html">
                            <img src="images/Emendi.png" alt="Homepage">
                        </a>
                    </div>

                    <a class="s-header__menu-toggle" href="#0"><span>Menu</span></a>
                </div> <!-- end s-header__block -->

                <nav class="s-header__nav">
    
                    <ul class="s-header__menu-links">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="requirements.html">Requirements</a></li>
                        <li><a href="research.html">Research</a></li>
                        <li><a href="algorithms.html">Algorithms</a></li>
                        <li><a href="ui_design.html">UI Design</a></li>
                        <li><a href="system_design.html">System Design</a></li>
                        <li class="current"><a href="implementation.html">Implementation</a></li>
                        <li><a href="testing.html">Testing</a></li>
                        <li><a href="evaluation.html">Evaluation</a></li>
                        <li><a href="appendices.html">Appendices</a></li>
                        <li><a href="blog.html">Blog</a></li>
                    </ul> <!-- s-header__menu-links -->

                    <div class="s-header__contact">
                        <!-- <a href="contact.html" class="btn btn--primary s-header__contact-btn">Let's Work Together</a>                         -->
                    </div> <!-- s-header__contact -->
    
                </nav> <!-- end s-header__nav -->

            </div> <!-- end s-header__inner -->

        </header> <!-- end s-header -->



        <!-- # site main content
        ================================================== -->
        <section id="content" class="s-content">

            <section class="s-pageheader pageheader">
                 <div class="row">
                     <div class="column xl-12">
                         <h1 class="page-title">
                             <span class="page-title__small-type text-pretitle">The Key Features</span>
                             Implementation
                         </h1>
                         
                     </div>
                 </div>
            </section> <!-- end pageheader --> 

            
            <section class="s-pagecontent pagecontent">  
                
                <!--<div class="row pagemedia">
                    <d class="column xl-12">
                        <figure class="page-media">                                
                            <img src="images/thumbs/about/about-1200.jpg" 
                                 srcset="images/thumbs/about/about-2400.jpg 2400w, 
                                         images/thumbs/about/about-1200.jpg 1200w, 
                                         images/thumbs/about/about-600.jpg 600w" sizes="(max-width: 2400px) 100vw, 2400px" alt="">
                        </figure>
                    </d>
                </div> <!-- end pagemedia --> 
                
                <div class="row width-narrower pagemain">
                    <div class="column xl-12"> 
                        <p>These are the key features that allow us to achieve all of our project goals. All the key features are explained and documented throughly below.</p>
                        <h2>User Input</h2>
                        <img src="images/1.png" alt="User Input" style="width:100%;height:auto;">
                        <p>There are fiev different parts of the user interface: text box, smart generate button, select emotion box, manual generate button and user interface being disabled during audio generation. We have separated the different parts of the application into different column. 
                        </p>

                        <h5>Text box</h5>
                        <p>The text box allows the user to input a piece of text using a keyboard. The inputted text will then be used to generate emotional audio.</p>
                            <img src="images/2.png" alt="User Input" style="width:100%;height:auto;">
                        
                        <h5>Smart Generate</h5>
                        <p>This button function contains two different processes. The first process is the sentiment analysis of the text input. This will involve calling different pretrained sentiment analysis models, combining their different scores and returning a concluded emotion. The second process will then utilise the stored text and concluded emotion to generate emotional audio.</p>
                        <img src="images/3.png" alt="User Input" style="width:100%;height:auto;">
                        
                        <h5>Select Emotion and Manual Generate</h5>
                        <p>The select emotion box and manual generate button are combined into a single function for manual generation. The select box allows the user to choose the emotion of the emotional audio based on their text input. The text and selected emotion will then be stored and utilised to generate the emotional audio.</p>

                        <p>To ensure that the audio generation process is not interrupted, we have disabled all the user interface elements until the process is complete. This is because in Streamlit, every button press will cause a page refresh, which could cause errors during audio generation.</p>
                        <img src="images/4.png" alt="User Input" style="width:100%;height:auto;">
                        
                        <h5>Disable user interface during audio generation</h5>
                        <p>For the functionality of disabling the buttons during audio generation, we have implemented different session states in Streamlit called <code>process_button_clicked</code>
                            and <code>generation_in_progress</code>. When the session state of <code>generation_in_progress</code> is true, the text box, different buttons and select box will be disabled due to the way that they are constructed through <code>disabled = buttons_disabled</code>
                            where <code>buttons_disabled = st.session_state["generation_in_progress"]</code>
                           </p>
                           <p>When there is user input in the text box and the smart generate button has been pressed, it will set <code>st.session_state["generation_in_progress"]</code> to True to disable all buttons and set <code>process_button_clicked</code> to True to start audio generation. After it has generated the audio, it will call <code>st.rerun()</code> to refresh the app and reset the session state flags to enable the user interface again.</p>
                           <img src="images/5.png" alt="User Input" style="width:100%;height:auto;">
                       
                        <p>When a button is pressed, <code>process_button_clicked</code> session state will be set to True. If the <code>selected_emotion</code> session state is not empty, it will mean that the user is chosen to manual generate with the selected emotion and text, hence call <code>AudioGenerator.generate_audio_with_selected_emotion(st.session_state["selected_emotion"], text)</code>. If the <code>selected_emotion</code> session state is empty or non-existent, it will call the smart generation function instead with <code>AudioGenerator.analyse_and_generate_audio(text)</code></p>
                        <img src="images/6.png" alt="User Input" style="width:100%;height:auto;">
                        <h2>Sentiment Analysis</h2>
                        <p>
                            We have imported and utilised pretrained sentiment analysis models to give us emotional indicators for text input. This function is used for the smart generation button since it needs to analyse the text and decide on a concluded emotion. We have initalised all the sentiment analysis models in <code>sentiment_analysis.py</code> to avoid reloading on each function call.
                        </p>          
                        <img src="images/13.png" alt="User Input" style="width:100%;height:auto;">
                        <p>Through research, we have found that the same text can yield different results from different sentiment analysis models. As a result, we utilised multiple models and combined their scores together to reach an increasingly accurate result. This enables us to reliably analyse the emotion of the text input.</p>       
                        
                        <p>The <code>AnalysisManager</code> class has many different functions, including returning and formatting the scores of each model in sorted order. This allows us to view the emotion with the top score for each model, which will be utilised in concluding the final emotion. </p>
                        <img src="images/14.png" alt="User Input" style="width:100%;height:auto;"> 

                        <p>Most importantly, the <code>conclude_emotion</code> function combines all the different results together and returns the concluded result.</p>
                        <p>Through research and testing for different emotional texts generated by ChatGPT, we have found that the DistilRoBERTa models are the most accurate, hence their result takes higher precedence.
                        </p>
                        <p>Furthermore, we found that the results from VADER is not very helpful in concluding the emotion. The result only returns positive, negative, netural and a compound score. Hence, these scores are redundant when comparing to other sentiment analysis models.</p>
                        <img src="images/11.png" alt="User Input" style="width:100%;height:auto;">
                        <h2>Emotional Audio Generation</h2>
                        <p>
                            In the <code>AudioGenerator</code> class, there are two functions <code>analyse_and_generate_audio</code>
                            and <code>generate_audio_with_selected_emotion</code> which is used for smart and manual audio generation. Both functions will call <code>generator.py</code>. In <code>generator.py</code> it will read from <code>results.json</code>, and temporarily write a new json file containing the contents of <code>results.json</code> in the VALL directory.
                            
                        </p>
                        <img src="images/7.png" alt="User Input" style="width:100%;height:auto;">
                        <p>It will then call another Python script called <code>voice_generator.py</code> which is located in the VALL directory</p>
                        <p>In <code>voice_generator.py</code>, it will call <code>preload_models()</code> to load the necessary audio generation models. It will then open and read from <code>temporary_file.json</code> which was the temporary file that was created in <code>generator.py</code> to replicate the data in <code>results.json</code>. 
                        </p>
                        <p>As a result, it has access to the input text and the concluded/select emotion. This allows us to generate the emotional audio by passing in text and the <code>concluded_emotion</code> as arguments in the <code>generate_audio</code> function in VALL-E-X model. Then we will write the generatde wav file to <code>output_wav</code> using the id accessed in <code>temporary_file.json</code>, which was the unique id generated from the hash function.</p>
                        <img src="images/8.png" alt="User Input" style="width:100%;height:auto;">
                        <h2>Audio Playback</h2>
                        <p>In <code>play_audio.py</code>, we have included the function of audio playback. Streamlit has built-in audio playback function using <code>st.audio()</code>. To identify the correct audio file, it will read results.json and search for the audio file in <code>output_wav</code> based on the unique id stored in <code>results.json</code> to build the correct audio file path. If the audio file is found, it will call <code>st.audio()</code> to play the audio.
                        </p>
                        <img src="images/12.png" alt="User Input" style="width:100%;height:auto;">
                        <h2>Hash Function to check for existing audio files</h2>
                        <p>This function utilises the input text and concluded/selected emotion combined as the input of hash function to generate an unique id. The unique id will then be used as the audio file name when generating the emotional audio. This is because if the user has the same repeated text of the same emotion, we can save computational resources by checking if the audio file has already been generated previously. If it has, it can reuse it again to be more computational efficient
                        </p>
                        <p>Since our system aims to help MND patients communicate efficiently, we should make conversational phrases be generated as quickly as possible. Even though the time of generation depends on the length of the text, we can include some pre-generated emotional audio that is extremely common in daily communications in the software package. Hence, when the user uses the phrases, hash function will generate an unique id that is identical to an already-downloaded audio file. This allows the user to communicate very efficiently without generating the phrases again.
                        </p>
                        <p><b>SHA-256</b> is specifically designed to minimise the likelihood of collisions by generating a unique hash value for each input with a high degree of probability. However, in practical terms, hash collisions can still occur due to factors such as the birthday paradox, where the probability of a collision increases as more hash values are generated.
                        </p>
                        <p>However, the likelihood of a hash collision occurring in a well-designed hash function like SHA-256 is extremely low, especially when considering the large number of possible hash values (2^256) compared to the total number of possible inputs. The birthday paradox suggests that in a set of n randomly chosen elements, there is a 50% chance of at least two elements having the same value when n is approximately the square root of the number of possible values. For SHA-256, with 2^256 possible hash values, the likelihood of a collision is very rare.
                        </p>
                        <img src="images/9.png" alt="User Input" style="width:100%;height:auto;">
                        <p>These two functions are then used in the <code>AudioGenerator</code> class to check whether or not we need to generate a new audio file. For instance, with the manual generate function, we will create an unique id based on the input text and select emotion. The unique id will then be passed to <code>Utility.check_audio_file_exists</code> function and returns a Boolean to determine if the audio file with the same name already exists in <code>output_wav</code>. If it returns True, the same text and emotion has already been generated as emotional audio before, hence there is no need to generate the same audio again.
                        </p>
                        <img src="images/10.png" alt="User Input" style="width:100%;height:auto;">
                        

                    </div> <!-- end grid-block-->
                </div> <!-- end pagemain -->

           </section> <!-- pagecontent -->

        <!-- # footer 
        ================================================== -->
        <footer class="s-footer">
            <div class="row s-footer__content">
                <div class="column xl-6 lg-6 md-12 s-footer__block s-footer__about">                    
                    <h3>About EMENDi2</h3>
                    <p>
                    EMENDi2 aims to significantly improve the communication processfor MND patients by introducing an emotional voice synthesiser. 
                    </p>               
                </div>                
                <div class="column xl-3 lg-6 md-12 s-footer__block s-footer__site-links">
                    
                </div>
                
            
            <div class="row s-footer__bottom">
                <div class="column xl-6 lg-12">
                    <ul class="link-list">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="requirements.html">Requirements</a></li>
                        <li><a href="research.html">Research</a></li>
                        <li><a href="algorithms.html">Algorithms</a></li>
                        <li><a href="ui_design.html">UI Design</a></li>
                        <li><a href="system_design.html">System Design</a></li>
                        <li><a href="implementation.html">Implementation</a></li>
                        <li><a href="testing.html">Testing</a></li>
                        <li><a href="evaluation.html">Evaluation</a></li>
                        <li><a href="appendices.html">Appendices</a></li>
                        <li><a href="blog.html">Blog</a></li>
                    </ul>
                </div>
                <div class="column xl-6 lg-12">
                    <p class="ss-copyright">
                        <!-- <span>Â© Copyright Monica 2022</span> 
                        <span>Design by <a href="https://www.styleshout.com/">StyleShout</a> Distribution By <a href="https://themewagon.com">ThemeWagon</a></span> -->
                    </p>
                </div>

                <div class="ss-go-top">
                    <a class="smoothscroll" title="Back to Top" href="#top">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="36" height="36" fill="none" stroke="#ffffff" stroke-width="1" stroke-linecap="round" stroke-linejoin="round">&lt;!--!  Atomicons Free 1.00 by @atisalab License - https://atomicons.com/license/ (Icons: CC BY 4.0) Copyright 2021 Atomicons --&gt;<polyline points="17 11 12 6 7 11"></polyline><line x1="12" y1="18" x2="12" y2="6"></line></svg>
                    </a>
                </div> <!-- end ss-go-top -->
            </div>
            
        </footer> <!-- end s-footer -->

    </div> <!-- end page-wrap -->
    <!-- Java Script
    ================================================== -->
    <script src="js/plugins.js"></script>
    <script src="js/main.js"></script>

</body>
</html>