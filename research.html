<!DOCTYPE html>
<html lang="en" class="no-js" >
<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Research</title>

    <script>
        document.documentElement.classList.remove('no-js');
        document.documentElement.classList.add('js');
    </script>

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="css/vendor.css">
    <link rel="stylesheet" href="css/styles.css">

    <!-- favicons
    ================================================== -->
    <link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="manifest" href="site.webmanifest">

</head>


<body id="top">

    
    <!-- preloader
    ================================================== -->
    <div id="preloader">
        <div id="loader" class="dots-fade">
            <div></div>
            <div></div>
            <div></div>
        </div>
    </div>


    <!-- page wrap
    ================================================== -->
    <div id="page" class="s-pagewrap ss-home">


        <!-- # site header 
        ================================================== -->
        <header class="s-header">

            <div class="row s-header__inner width-sixteen-col">

                <div class="s-header__block">
                    <div class="s-header__logo">
                        <a class="logo" href="index.html">
                            <img src="images/Emendi.png" alt="Homepage">
                        </a>
                    </div>

                    <a class="s-header__menu-toggle" href="#0"><span>Menu</span></a>
                </div> <!-- end s-header__block -->

                <nav class="s-header__nav">
    
                    <ul class="s-header__menu-links">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="requirements.html">Requirements</a></li>
                        <li class="current"><a href="research.html">Research</a></li>
                        <li><a href="algorithms.html">Algorithms</a></li>
                        <li><a href="ui_design.html">UI Design</a></li>
                        <li><a href="system_design.html">System Design</a></li>
                        <li><a href="implementation.html">Implementation</a></li>
                        <li><a href="testing.html">Testing</a></li>
                        <li><a href="evaluation.html">Evaluation</a></li>
                        <li><a href="appendices.html">Appendices</a></li>
                        <li><a href="blog.html">Blog</a></li>
                    </ul> <!-- s-header__menu-links -->

                    <div class="s-header__contact">
                        <!-- <a href="contact.html" class="btn btn--primary s-header__contact-btn">Let's Work Together</a>                         -->
                    </div> <!-- s-header__contact -->
    
                </nav> <!-- end s-header__nav -->

            </div> <!-- end s-header__inner -->

        </header> <!-- end s-header -->
        <!-- # site main content
        ================================================== -->
        <section id="content" class="s-content">

            <section class="s-pageheader pageheader">
                 <div class="row">
                     <div class="column xl-12">
                         <h1 class="page-title">
                             <span class="page-title__small-type text-pretitle">Research Process</span>
                             Research
                         </h1>
                         
                     </div>
                 </div>
            </section> <!-- end pageheader --> 

            <p> </p>
                <div class="row width-narrower pagemain">
                    <div class="column xl-12"> 

                        <h2>Related Project Review</h2>
                        <div>
                            <p>The main component of this project is the output of emotional audio. 
                            This is very similar to existing applications such as <b>WavelAI</b> and <b>TypecastAI</b> since both are designed to generate emotional audio from text input. 
                            As both the products are business solutions with a monthly service fee, the emotional audio output service are all private. Hence, the main difference between these products and our project is that our project is implemented with only open-sourced models and libraries.
                            This significant difference allows us to achieve one of the main goals of our project, which is to provide this service to MND patients without any financial barriers.
                            </p>
                        </div>

                        <div>
                            <p>
                                <b>Tobii Dynavox</b> is a company specialising in asistive technology for communication such as communication apps, speech-generating devices and eye trackers for people with disabilities.
                                It is similar to our project since it also has developed devices that can generate speech
                            </p>
                            <p>One of their applications is <b>TD Talk [1]</b>, which supports speech generation for literate adults with communication disabilities using eyes or fingers. The application offers a wide selection of languages and voices, predictive sentence construction, easy editing, and the Partner Window for showing typed text. 
                                <a class="images"> <img src="images/tdtalk.webp" alt="Homepage"></a>
                            </p>
                            <p>Another one of their applications is <b>Communicator 5 [2]</b>, which allows for text-to-speech conversion using eyes, fingers, or a switch, with added functionalities for internet access and environmental control. 
                            </p>
                            
                            
                            <p>From existing applications, we can learn the importance:
                                <ul>
                                    <p><li><b>User Interface</b>: The UI must be tailored to the specific needs of the users, especially considering the physical limitations of individuals with conditions like MND. The ability to use eye-tracking for communication is particularly valuable. Although our current project does not include any eye-tracking, a future improvement of the UI could be achieved through collaboration with eye-tracking MotionInput teams</li></p>
                                    <p></p><li><b>Accessibility</b>: Accessibility features are crucial for inclusivity, ensuring that individuals with varying degrees of motor abilities can use the application effectively. The main target of our project is MND patients but it would be extremely impactful if it could be utilised for other speech impairment diseases.
                                    </li></p>
                                    <li>
                                        <b>Integration</b>: The software's ability to integrate with various input methods and platforms (like the internet, social media, and home control systems) showcases the importance of a flexible and extensible system.
                                    </li>
                                </ul> 
                        </div>
                    
                        


                        <h2>Technology Review</h2> 
                        <p>After creating the MoSCow list and identifying the main aims of the project, we researched technologies that we could implement to help us achieve our goals.</p>
                        <p>Our project's main aim is to infuse spoken audio generated from text with emotion on a reasonable time scale. This involves building on existing TTS models, as training such a model from scratch would require immense amounts of data and computing resources. There are various methods available to achieve this task, and some of them are described below.  
                        </p>

                        
                        
                        <ul>
                            <p><li><b>Fine-tuning</b>: Fine-tuning is a process that makes use of a pre-trained model's more general capabilities and tunes its responses in a specific context that matches our needs. This method is useful as it requires less computational power, training data as well as time required for training. This makes the method a viable option for our project on the given time scale. 
                            </li></p>
                            <p><li><b>Audio Presets</b>: Existing open source TTS models often allow the use of voice presets that can make changes to voice, pitch and prosody levels in the default model output. These metrics are key to affecting emotion in speech, and thus can be leveraged by configuring voice presets for each emotion during development. 
                            </li></p>
                            <p><li><b>Re-training</b>:
                                Re-training existing TTS models with modifications to the model architecture allows the incorporation of pitch and prosodic changes that weren't originally possible. For our project, this would mean introducing speaker embeddings and emotion IDs to a corpus of audio data transcribed and labelled with the corresponding emotion. However, this method requires access to significantly powerful hardware and unrealistic time scales in this context. 
                    
                                </li></p>
                                <p><li><b>Prompt Conditioning</b>:
                                    Prompt conditioning involves making use of prompts visible to the model during text processing and audio generation that allow dynamic control over natural speech patterns, like pauses. Depending on the specific model capabilities due to the data available to train it on, this can range from introducing convincing human laughter to potentially attuning the audio to a specific emotion with curated prompts.
                                    </li></p>
                            
                        </ul>

                        Each methodology described above comes with its own pros and cons. 
                        <table>
                            <tr>
                              <th>Method</th>
                              <th>Explanation</th>
                            </tr>
                            <tr>
                              <td>Fine-tuning</td>
                              <td>While this method significantly decreases the time required to obtain a model that achieves one's desired goal, the amount of highly specific data required to implement this method effectively remains a barrier.
                            </td>
                            </tr>
                            <tr>
                                <td>Audio presets</td>
                                <td>This method, although restrictive in the range of emotion available to synthesise, allows custom audio generation with a lower time period required to process text and generate input.</td>
                              </tr>
                              <tr>
                                <td>Re-training</td>
                                <td>This method allows the most control over the structure of the model and the output produced. However, not only does it require a large amount of data in order to achieve a usable standard, it also takes much more time than the other methods described. Due to the nature of our project's timeline, this method is less feasible than others.
                                </td>
                              </tr>
                              <tr>
                                <td>Prompt-conditioning</td>
                                <td>While flexible, one potential drawback is that prompt processing generally takes up more compute power, as well as an increased amount of time between input and output. 
                                </td>
                              </tr>
                          </table>
                          
                        <h4>Programming Language</h4>
                            <p>For machine learning, <b>Python</b> is one of the best programming languages to use due to its rich ecosystem of advanced libraries and frameworks such as TensorFlow, PyTorch and scikit-learn. These provide comprehensive tools for building and deploying ML models efficiently. Additionally, due to its ML capacities, many sentiment analysis and emotional audio generation models are in Python.
                            </p>
                            <p>While Python itself may not be as fast as lower-level languages like C or C++, its performance can be enhanced through the use of optimised libraries and techniques. Additionally, Python interfaces seamlessly with high-performance computing libraries and frameworks, enabling scalable machine learning solutions.
                                </p>                      
                        <h4>GUI and frontend</h4>
                        <p>We will create an intuitive user interface that allows the users to easily select the text and emotions. Due to deciding to use Python for machine learning purposes, it is appropriate to use Python for the GUI as well.</p>
                            <p>For our GUI, we considered many different toolkits that can suit our requirement of creating a simple and easy-to-use GUI for MND patients to generate emotional audio. Some common Python libraries are <b>PyQt</b> and <b>Tkinter</b>. Despite both having lots of benefits such as Tkinter being easy to use and able to run on any platform that supports Python, and PyQt being highly customisable, we decided to use <b>Streamlit</b>. 
                            </p>
                            <p>The main reason is that Streamlit provides an easy way to handle user input and interactivity without the need for a complex event-handling system, which is vital to reducing the complexity of our system’s text input and audio playback functionality. Additionally, it has seamless integration with Python data science libraries, which would be useful for further developments</p>
                        <table>
                            <tr>
                              <th>GUI</th>
                              <th>Advantages</th>
                            </tr>
                            <tr>
                              <td>Tkinter</td>
                              <td>Easy to use and runs on any platform that has Python installed since it is included with Python by default (no need for additional libraries or dependencies).

                            </td>
                            </tr>
                            <tr>
                                <td>PyQt</td>
                                <td> Rich GUI features and has board cross-platform compatibility between different OS
                                    .</td>
                              </tr>
                              <tr>
                                <td>Streamlit</td>
                                <td>Applications can be easily deployed and shared online, allowing for easy accessibility. Streamlit applications are also highly customisable and can be integrated with Python data science libraries.</td>
                              </tr>
                              
                          </table>

                          <table>
                            <tr>
                              <th>GUI</th>
                              <th>Disadvantages</th>
                            </tr>
                            <tr>
                              <td>Tkinter</td>
                              <td>Dull and outdated appearance compared to more modern GUIs. Furthermore, it has limited customisations of widgets without extensive coding.

                            </td>
                            </tr>
                            <tr>
                                <td>PyQt</td>
                                <td>  It has a complex API. Additionally, PyQt applications can be quite large after packaging for deployment because they include the large Qt libraries.

                                    </td>
                              </tr>
                              <tr>
                                <td>Streamlit</td>
                                <td>Web only since it is designed for deploying web applications. 
                                    </td>
                              </tr>
                              
                          </table>
                            <h4>Sentiment Analysis</h4>  
                            <p>There are many open-sourced sentiment analysis models available online. We have researched and utilised different sentiment analysis models that enable us to very accurately analyse the emotion of text. We have listed them below, combined with their advantages and disadvantages</p>
                            <p><b>BERT and DistilBERT [3,4]</b></p>
                            <ul>
                                <p><li>
                                    <span style="color: green;">Bidirectional Context</span>: BERT's bidirectional architecture allows it to capture contextual information from both left and right contexts, enabling a deeper understanding of language semantics and syntax.
                                    </li></p>
                                <p><li><span style="color: green;">Transfer Learning</span>: BERT's pre-trained representations can be fine-tuned on specific downstream tasks with relatively small amounts of task-specific data, making it highly adaptable and versatile.</li></p>
                                
                                <li><span style="color: red;">Limited Emotions</span>: Does not include disgust and neutral emotion. Also it is not very accurate for the surprise emotion through many experimentations.
                                </li>
                            </ul>
                            <p><b>VADER</b></p>
                            <ul>
                                <p><li>
                                    <span style="color: green;">Pre-Trained for Sentiment Analysis</span>: VADER is specifically designed for sentiment analysis, making it easy to implement for tasks such as sentiment classification and polarity detection.                                    
                                    </li></p>
                                <p><li><span style="color: green;">Fast and Lightweight</span>: VADER is relatively lightweight and executes quickly, making it suitable for real-time sentiment analysis applications.                                </li></p>
                                <p><li><span style="color: green;">Rule-Based</span>: VADER uses a lexicon and rule-based approach, which can be advantageous for certain types of sentiment analysis tasks, especially when dealing with social media or informal text.
                                </li></p>
                                <li><span style="color: red;">Limited Context Understanding</span>: VADER's rule-based approach may struggle with understanding complex language nuances and sarcasm, leading to less accurate sentiment predictions in some cases.

                                </li>
                            </ul>
                            <p><b>DistilRoBERTa [5,6]</b></p>
                            <ul>
                                <p><li>
                                    <span style="color: green;">High Performance</span>:DistilRoBERTa is based on the RoBERTa architecture, which is known for its strong performance across various natural language processing (NLP) tasks, including sentiment analysis, text classification, and more.
                                    </li></p>
                                <p><li><span style="color: green;">Efficient</span>: DistilRoBERTa is a distilled version of RoBERTa, meaning it retains much of the original model's performance while being more resource-efficient in terms of memory and computation.
                                </li></p>
                                
                                <li><span style="color: red;">Resource Intensive</span>: While more efficient than the full-sized RoBERTa model, DistilRoBERTa still requires significant computational resources, especially during fine-tuning or inference on large datasets.
                                    Also it is not very accurate for the surprise emotion through many experimentations.
                                </li>
                            </ul>
                            <p>Each sentiment analysis model has its own advantages and disadvantages. Hence, we will implement an algorithm that will analyse the results from each model and conclude on a single emotion, considering all edge cases and scores.
                            </p>
                            
                        <h4>Emotional Audio Generation</h4>
                            <p><u>Possible Technology: Emotivoice [7]</u></p>
                            <p>The TTS model EmotiVoice is a model that offers the capabilities required for our main goals. As a prompt-controlled TTS engine, it allows a degree of flexibility in the audio produced by the model, such as emphasis of tone or subtle pauses.
                            </p>
                            <p>Crucially, it takes one of six emotions (Calm, Happy, Angry, Fearful, Surprised, Disgusted) along with the phonemes that correspond to the text input in order to render the audio as required. The model architecture is based on the PromptTTS research paper by Microsoft, which describes the ability to make use of embedded prompts in the input text to influence prosody in a more dynamic way.</p>
                            <p>For our project, not only can we make use of the EmotiVoice TTS engine, we can also take inspiration from its basis rooted in prompt-controlled audio generation in order to produce convincing emotional audio.
                            </p>
                            <p><u>Our Technology: VALL-E-X [8]</u></p>
                                  
                        
                        <h2>Comparison between different libraries</h2>
                        <h2>Summary of Our Technical Decisions</h2>
                        <table>
                            <tr>
                              <th>Technical Problem</th>
                              <th>Our Decision</th>
                            </tr>
                            <tr>
                              <td>Programming Language (front and back end)</td>
                              <td>Python 3</td>
                            </tr>
                            <tr>
                              <td>Sentiment Analysis</td>
                              <td>BERT, VADER and DistilRoBERTa models</td>
                            </tr>
                            <!-- ... Add the other rows similarly -->
                            <tr>
                              <td>GUI</td>
                              <td>Streamlit</td>
                            </tr>
                            <tr>
                              <td>Emotional Audio Generation</td>
                              <td>Vall-E-X</td>
                            </tr>
                          </table>

                          <h2>References</h2>
                          <ol>
                              <li>
                                  TD Talk. [Online]. Available: <a href="https://uk.tobiidynavox.com/products/td-talk?tab=1">https://uk.tobiidynavox.com/products/td-talk?tab=1</a>
                              </li>
                              <li>
                                  Communicator 5. [Online]. Available: <a href="https://uk.tobiidynavox.com/pages/communicator-5">https://uk.tobiidynavox.com/pages/communicator-5</a>
                              </li>
                              <li>
                                  Savani, B. (2022). DistilBERT. [Online]. Available: <a href="https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion">https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion</a>
                              </li>
                              <li>
                                  NLPTown. (2022). BERT. [Online]. Available: <a href="https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment">https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment</a>
                              </li>
                              <li>
                                  Hartmann, J. (2024). DistilRoBERTa. [Online]. Available: <a href="https://huggingface.co/j-hartmann/emotion-english-distilroberta-base">https://huggingface.co/j-hartmann/emotion-english-distilroberta-base</a>
                              </li>
                              <li>
                                Jieli, M. (2022). DistilRoBERTa. [Online]. Available <a href="https://huggingface.co/michellejieli/emotion_text_classifier">https://huggingface.co/michellejieli/emotion_text_classifier</a>
                              </li>
                              <li>
                                  netease-youdao. (2022). EmotiVoice. [Online]. Available: <a href="https://github.com/netease-youdao/EmotiVoice">https://github.com/netease-youdao/EmotiVoice</a>
                              </li>
                              <li>
                                  Plachtaa. (2020). VALL-E-X. [Online]. Available: <a href="https://github.com/Plachtaa/VALL-E-X">https://github.com/Plachtaa/VALL-E-X</a>
                              </li>
                          </ol>
                          
                    </div> <!-- end grid-block-->
                </div> <!-- end pagemain -->

           </section> <!-- pagecontent -->

                 <!-- # footer 
        ================================================== -->
        <footer class="s-footer">
            <div class="row s-footer__content">
                <div class="column xl-6 lg-6 md-12 s-footer__block s-footer__about">                    
                    <h3>About EMENDi2</h3>
                    <p>
                    EMENDi2 aims to significantly improve the communication processfor MND patients by introducing an emotional voice synthesiser. 
                    </p>               
                </div>                
                <div class="column xl-3 lg-6 md-12 s-footer__block s-footer__site-links">
                    
                </div>
                
            
            <div class="row s-footer__bottom">
                <div class="column xl-6 lg-12">
                    <ul class="link-list">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="requirements.html">Requirements</a></li>
                        <li><a href="research.html">Research</a></li>
                        <li><a href="algorithms.html">Algorithms</a></li>
                        <li><a href="ui_design.html">UI Design</a></li>
                        <li><a href="system_design.html">System Design</a></li>
                        <li><a href="implementation.html">Implementation</a></li>
                        <li><a href="testing.html">Testing</a></li>
                        <li><a href="evaluation.html">Evaluation</a></li>
                        <li><a href="appendices.html">Appendices</a></li>
                        <li><a href="blog.html">Blog</a></li>
                    </ul>
                </div>
                <div class="column xl-6 lg-12">
                    <p class="ss-copyright">
                        <!-- <span>© Copyright Monica 2022</span> 
                        <span>Design by <a href="https://www.styleshout.com/">StyleShout</a> Distribution By <a href="https://themewagon.com">ThemeWagon</a></span> -->
                    </p>
                </div>

                <div class="ss-go-top">
                    <a class="smoothscroll" title="Back to Top" href="#top">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="36" height="36" fill="none" stroke="#ffffff" stroke-width="1" stroke-linecap="round" stroke-linejoin="round">&lt;!--!  Atomicons Free 1.00 by @atisalab License - https://atomicons.com/license/ (Icons: CC BY 4.0) Copyright 2021 Atomicons --&gt;<polyline points="17 11 12 6 7 11"></polyline><line x1="12" y1="18" x2="12" y2="6"></line></svg>
                    </a>
                </div> <!-- end ss-go-top -->
            </div>
            
        </footer> <!-- end s-footer -->

    </div> <!-- end page-wrap -->


    </div> <!-- end page-wrap -->
    <!-- Java Script
    ================================================== -->
    <script src="js/plugins.js"></script>
    <script src="js/main.js"></script>

</body>
</html>